{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraer información de los json\n",
    "Estructura del json:\n",
    "```json\n",
    "{\n",
    "    \"links\": [],\n",
    "    \"metadata\": {},\n",
    "    \"items\": [\n",
    "        {\n",
    "            ...\n",
    "            \"title\": \"titulo del articulo\",\n",
    "            \"abstractText\": [\"abstract del articulo\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombres de los json:\n",
    "\n",
    "### editorial.json\n",
    "Contiene: --- <br>\n",
    "Necesitamos: 153\n",
    "\n",
    "### book_chapter.json\n",
    "Contiene: 200 <br>\n",
    "Necesitamos: 122\n",
    "\n",
    "### review.json\n",
    "Contiene: 200 <br>\n",
    "Necesitamos 99\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia de exacción\n",
    "Sacar la cantidad de items necesarios, sacando el nombre y el abstract para colocarlo en un dataset de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracción de datos de reviews\n",
    "def extract_info_from_json(file_path, target_count):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Filtrar elementos que tienen 'abstractText' no vacío\n",
    "    filtered_items = [item for item in data['items'] if 'abstractText' in item and item['abstractText']]\n",
    "    # Limitar a target_count si hay suficientes elementos\n",
    "    limited_items = filtered_items[:target_count]\n",
    "    extracted_info = [\n",
    "        {\n",
    "            'title': item['title'],\n",
    "            'abstractText': ' '.join(item['abstractText']),\n",
    "            'source': file_path  # Añadir el nombre del archivo como parte de la información extraída\n",
    "        }\n",
    "        for item in limited_items\n",
    "    ]\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los archivos y la cantidad de items a extraer\n",
    "files_and_items = {\n",
    "    'editorial.json': 153,\n",
    "    'book_chapter.json': 122,\n",
    "    'review.json': 99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la información y convertirla en DataFrames\n",
    "dataframes = []\n",
    "for file_path, num_items in files_and_items.items():\n",
    "    info = extract_info_from_json(file_path, num_items)\n",
    "    df = pd.DataFrame(info)\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los DataFrames en uno solo\n",
    "final_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vladimircuriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descargar la lista de stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las stop words en inglés\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar el texto\n",
    "def clean_text(text):\n",
    "    # Eliminar etiquetas HTML\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Quitar signos de puntuación\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Dividir el texto en palabras\n",
    "    words = word_tokenize(text)\n",
    "    # Filtrar las stop words\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    # Unir las palabras filtradas de nuevo en una cadena\n",
    "    text = ' '.join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función de limpieza a las columnas deseadas\n",
    "final_df['abstractText'] = final_df['abstractText'].apply(clean_text)\n",
    "final_df['title'] = final_df['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstractText</th>\n",
       "      <th>source</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>augmenting research methods foundation models ...</td>\n",
       "      <td>deep learning dl research made remarkable prog...</td>\n",
       "      <td>editorial.json</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>special issue soco 2022 new trends soft comput...</td>\n",
       "      <td>eight papers included special issue represent ...</td>\n",
       "      <td>editorial.json</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>computational models cognition humanautomated ...</td>\n",
       "      <td>discuss stateoftheart future directions develo...</td>\n",
       "      <td>editorial.json</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>special issue soco 2021 new trends soft comput...</td>\n",
       "      <td>seven papers included special issue represent ...</td>\n",
       "      <td>editorial.json</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>biomedical informatics state art challenges op...</td>\n",
       "      <td>biomedical informatics considered multidiscipl...</td>\n",
       "      <td>editorial.json</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>bioinspired algorithms feature engineering ana...</td>\n",
       "      <td>purpose natures evolution shaped intelligent b...</td>\n",
       "      <td>review.json</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>computer computer science computational thinki...</td>\n",
       "      <td>digital computers invented 1940s sophisticated...</td>\n",
       "      <td>review.json</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>navigating metaverse technical review emerging...</td>\n",
       "      <td>metaverse burgeoning virtual reality realm gar...</td>\n",
       "      <td>review.json</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>human activity prediction studies using wearab...</td>\n",
       "      <td>nowadays human activity recognition har system...</td>\n",
       "      <td>review.json</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>predictive analytics emotional recognitionthe ...</td>\n",
       "      <td>paper explores fusion data science cognitive t...</td>\n",
       "      <td>review.json</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    augmenting research methods foundation models ...   \n",
       "1    special issue soco 2022 new trends soft comput...   \n",
       "2    computational models cognition humanautomated ...   \n",
       "3    special issue soco 2021 new trends soft comput...   \n",
       "4    biomedical informatics state art challenges op...   \n",
       "..                                                 ...   \n",
       "346  bioinspired algorithms feature engineering ana...   \n",
       "347  computer computer science computational thinki...   \n",
       "348  navigating metaverse technical review emerging...   \n",
       "349  human activity prediction studies using wearab...   \n",
       "350  predictive analytics emotional recognitionthe ...   \n",
       "\n",
       "                                          abstractText          source  \\\n",
       "0    deep learning dl research made remarkable prog...  editorial.json   \n",
       "1    eight papers included special issue represent ...  editorial.json   \n",
       "2    discuss stateoftheart future directions develo...  editorial.json   \n",
       "3    seven papers included special issue represent ...  editorial.json   \n",
       "4    biomedical informatics considered multidiscipl...  editorial.json   \n",
       "..                                                 ...             ...   \n",
       "346  purpose natures evolution shaped intelligent b...     review.json   \n",
       "347  digital computers invented 1940s sophisticated...     review.json   \n",
       "348  metaverse burgeoning virtual reality realm gar...     review.json   \n",
       "349  nowadays human activity recognition har system...     review.json   \n",
       "350  paper explores fusion data science cognitive t...     review.json   \n",
       "\n",
       "     word_count  \n",
       "0           127  \n",
       "1            72  \n",
       "2           118  \n",
       "3            72  \n",
       "4            86  \n",
       "..          ...  \n",
       "346         165  \n",
       "347         113  \n",
       "348         162  \n",
       "349         105  \n",
       "350         124  \n",
       "\n",
       "[351 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el DataFrame final\n",
    "final_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo top 10 palabras comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar el texto de las columnas 'title' y 'abstractText' en una sola serie de texto\n",
    "all_text = final_df['abstractText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el texto en palabras y aplanar la lista\n",
    "words = word_tokenize(' '.join(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la frecuencia de cada palabra\n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar las 10 palabras más comunes\n",
    "most_common_words = word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science: 415\n",
      "data: 342\n",
      "learning: 332\n",
      "research: 314\n",
      "computer: 303\n",
      "ai: 227\n",
      "systems: 167\n",
      "applications: 145\n",
      "intelligence: 143\n",
      "field: 143\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las 10 palabras más comunes y sus frecuencias\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo el promedio de la cantidad de palabras de los abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular la cantidad de palabras en cada abstract\n",
    "final_df['word_count'] = final_df['abstractText'].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el promedio de estas cantidades\n",
    "average_word_count = final_df['word_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de palabras por abstract es: 105\n"
     ]
    }
   ],
   "source": [
    "print(f\"El promedio de palabras por abstract es: {math.ceil(average_word_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el DataFrame final\n",
    "final_df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
